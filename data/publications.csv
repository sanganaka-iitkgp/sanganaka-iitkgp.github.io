title,abstract,author,issued,type,type-name,link
Hate begets hate: A temporal study of hate speech,"With the ongoing debate on 'freedom of speech' vs. 'hate speech,' there is an urgent need to carefully understand the consequences of the inevitable culmination of the two, i.e., 'freedom of hate speech' over time. An ideal scenario to understand this would be to observe the effects of hate speech in an (almost) unrestricted environment. Hence, we perform the first temporal analysis of hate speech on Gab.com, a social media site with very loose moderation policy. We first generate temporal snapshots of Gab from millions of posts and users. Using these temporal snapshots, we compute an activity vector based on DeGroot model to identify hateful users. The amount of hate speech in Gab is steadily increasing and the new users are becoming hateful at an increased and faster rate. Further, our analysis analysis reveals that the hate users are occupying the prominent positions in the Gab network. Also, the language used by the community as a whole seem to correlate more with that of the hateful users as compared to the non-hateful ones. We discuss how, many crucial design questions in CSCW open up from our work.","Binny Mathew, Anurag Illendula, Punyajoy Saha, Soumya Sarkar, Pawan Goyal, Animesh Mukherjee",2020/10/14,Journal,Proceedings of the ACM on Human-Computer Interaction,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:OcBU2YAGkTUC
Using communities of words derived from multilingual word vectors for cross-language information retrieval in Indian languages,"We investigate the use of word embeddings for query translation to improve precision in cross-language information retrieval (CLIR). Word vectors represent words in a distributional space such that syntactically or semantically similar words are close to each other in this space. Multilingual word embeddings are constructed in such a way that similar words across languages have similar vector representations. We explore the effective use of bilingual and multilingual word embeddings learned from comparable corpora of Indic languages to the task of CLIR.We propose a clustering method based on the multilingual word vectors to group similar words across languages. For this we construct a graph with words from multiple languages as nodes and with edges connecting words with similar vectors. We use the Louvain method for community detection to find communities in this graph. We show that choosing target language words as query translations from the clusters or communities containing the query terms helps in improving CLIR. We also find that better-quality query translations are obtained when words from more languages are used to do the clustering even when the additional languages are neither the source nor the target languages. This is probably because having more similar words across multiple languages helps define well-defined dense subclusters that help us obtain precise query translations.In this article, we demonstrate the use of multilingual word embeddings and word clusters for CLIR involving Indic languages. We also make available a tool for obtaining related words and the visualizations of the multilingual word vectors for English, Hindi, Bengali, Marathi, Gujarati, and Tamil.","Paheli Bhattacharya, Pawan Goyal, Sudeshna Sarkar",2018/12/17,Journal,ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP),https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:Z5m8FVwuT1cC
Using word embeddings for query translation for hindi to english cross language information retrieval,"Cross-Language Information Retrieval (CLIR) has become an important problem to solve in the recent years due to the growth of content in multiple languages in the Web. One of the standard methods is to use query translation from source to target language. In this paper, we propose an approach based on word embeddings, a method that captures contextual clues for a particular word in the source language and gives those words as translations that occur in a similar context in the target language. Once we obtain the word embeddings of the source and target language pairs, we learn a projection from source to target word embeddings, making use of a dictionary with word translation pairs. We then propose various methods of query translation and aggregation. The advantage of this approach is that it does not require the corpora to be aligned (which is difficult to obtain for resource-scarce languages), a dictionary with word translation pairs is enough to train the word vectors for translation. We experiment with Forum for Information Retrieval and Evaluation (FIRE) 2008 and 2012 datasets for Hindi to English CLIR. The proposed word embedding based approach outperforms the basic dictionary based approach by 70% and when the word embeddings are combined with the dictionary, the hybrid approach beats the baseline dictionary based method by 77%. It outperforms the English monolingual baseline by 15%, when combined with the translations obtained from Google Translate and Dictionary.","Paheli Bhattacharya, Pawan Goyal, Sudeshna Sarkar",2016/9,Journal,Computación y Sistemas,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:mvPsJ3kp5DgC
A little pretraining goes a long way: A case study on dependency parsing task for low-resource morphologically rich languages,"Neural dependency parsing has achieved remarkable performance for many domains and languages. The bottleneck of massive labeled data limits the effectiveness of these approaches for low resource languages. In this work, we focus on dependency parsing for morphological rich languages (MRLs) in a low-resource setting. Although morphological information is essential for the dependency parsing task, the morphological disambiguation and lack of powerful analyzers pose challenges to get this information for MRLs. To address these challenges, we propose simple auxiliary tasks for pretraining. We perform experiments on 10 MRLs in low-resource settings to measure the efficacy of our proposed pretraining method and observe an average absolute gain of 2 points (UAS) and 3.6 points (LAS). Code and data available at: https://github.com/jivnesh/LCM","Jivnesh Sandhan, Amrith Krishna, Ashim Gupta, Laxmidhar Behera, Pawan Goyal",2021/2/12,Journal,arXiv preprint arXiv:2102.06551,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:fbc8zXXH2BUC
Query translation for cross-language information retrieval using multilingual word clusters,"In Cross-Language Information Retrieval, finding the appropriate translation of the source language query has always been a difficult problem to solve. We propose a technique towards solving this problem with the help of multilingual word clusters obtained from multilingual word embeddings. We use word embeddings of the languages projected to a common vector space on which a community-detection algorithm is applied to find clusters such that words that represent the same concept from different languages fall in the same group. We utilize these multilingual word clusters to perform query translation for Cross-Language Information Retrieval for three languages-English, Hindi and Bengali. We have experimented with the FIRE 2012 and Wikipedia datasets and have shown improvements over several standard methods like dictionary-based method, a transliteration-based model and Google Translate.","Paheli Bhattacharya, Pawan Goyal, Sudeshna Sarkar",2016/12,Conference,Proceedings of the 6th Workshop on South and Southeast Asian Natural Language Processing (WSSANLP2016),https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:PELIpwtuRlgC
Prabhupadavani: A code-mixed speech translation data for 25 languages,"Nowadays, the interest in code-mixing has become ubiquitous in Natural Language Processing (NLP); however, not much attention has been given to address this phenomenon for Speech Translation (ST) task. This can be solely attributed to the lack of code-mixed ST task labelled data. Thus, we introduce Prabhupadavani, which is a multilingual code-mixed ST dataset for 25 languages. It is multi-domain, covers ten language families, containing 94 hours of speech by 130+ speakers, manually aligned with corresponding text in the target language. The Prabhupadavani is about Vedic culture and heritage from Indic literature, where code-switching in the case of quotation from literature is important in the context of humanities teaching. To the best of our knowledge, Prabhupadvani is the first multi-lingual code-mixed ST dataset available in the ST literature. This data also can be used for a code-mixed machine translation task. All the dataset can be accessed at https://github.com/frozentoad9/CMST.","Jivnesh Sandhan, Ayush Daksh, Om Adideva Paranjay, Laxmidhar Behera, Pawan Goyal",2022/1/27,Journal,arXiv preprint arXiv:2201.11391,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:43bX7VzcjpAC
Meta-ed: Cross-lingual event detection using meta-learning for indian languages,"Lack of annotated data is a major concern in Event Detection (ED) tasks for low-resource languages. Cross-lingual ED seeks to address this issue by transferring information across various languages to improve overall performance. In this article, we propose a method for cross-lingual ED with a few training instances. We present a model agnostic meta-learning approach for few-shot cross-lingual ED that is able to find good parameter initialization and enables fast adaptation to new low-resource languages. We evaluate our model on four Indian languages. The results show that our approach significantly outperforms the base model.","Aniruddha Roy, Isha Sharma, Sudeshna Sarkar, Pawan Goyal",2023/2/21,Journal,ACM Transactions on Asian and Low-Resource Language Information Processing,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:MhiOAD_qIWkC
Event argument extraction using causal knowledge structures,"Event Argument extraction refers to the task of extracting structured information from unstructured text for a particular event of interest. The existing works exhibit poor capabilities to extract causal event arguments like Reason and After Effects. Furthermore, most of the existing works model this task at a sentence level, restricting the context to a local scope. While it may be effective for short spans of text, for longer bodies of text such as news articles, it has often been observed that the arguments for an event do not necessarily occur in the same sentence as that containing an event trigger. To tackle the issue of argument scattering across sentences, the use of global context becomes imperative in this task. In our work, we propose an external knowledge aided approach to infuse document-level event information to aid the extraction of complex event arguments. We develop a causal network for our event-annotated dataset by extracting relevant event causal structures from ConceptNet and phrases from Wikipedia. We use the extracted event causal features in a bi-directional transformer encoder to effectively capture long-range inter-sentence dependencies. We report the effectiveness of our proposed approach through both qualitative and quantitative analysis. In this task, we establish our findings on an event annotated dataset in 5 Indian languages. This dataset adds further complexity to the task by labelling arguments of entity type (like Time, Place) as well as more complex argument types (like Reason, After-Effect). Our approach achieves state-of-the-art performance across all the five languages. Since our work does not rely on any language-specific features, it can be easily extended to other languages.","Debanjana Kar, Sudeshna Sarkar, Pawan Goyal",2021/5/2,Journal,arXiv preprint arXiv:2105.00477,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:gKiMpY-AVTkC
Fully contextualized biomedical NER,"Recently, neural network architectures have outperformed traditional methods in biomedical named entity recognition. Borrowed from innovations in general text NER, these models fail to address two important problems of polysemy and usage of acronyms across biomedical text. We hypothesize that using a fully-contextualized model that uses contextualized representations along with context dependent transition scores in CRF can alleviate this issue and help further boost the tagger’s performance. Our experiments with this architecture have shown to improve state-of-the-art F1 score on 3 widely used biomedical corpora for NER. We also perform analysis to understand the specific cases where our contextualized model is superior to a strong baselin","Ashim Gupta, Pawan Goyal, Sudeshna Sarkar, Mahanandeeshwar Gattu",2019/4/7,Book,European Conference on Information Retrieval,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:L7CI7m0gUJcC
Enhancing low-resource nmt with a multilingual encoder and knowledge distillation: A case study,"Neural Machine Translation (NMT) remains a formidable challenge, especially when dealing with low-resource languages. Pre-trained sequence-to-sequence (seq2seq) multi-lingual models, such as mBART-50, have demonstrated impressive performance in various low-resource NMT tasks. However, their pre-training has been confined to 50 languages, leaving out support for numerous low-resource languages, particularly those spoken in the Indian subcontinent. Expanding mBART-50's language support requires complex pre-training, risking performance decline due to catastrophic forgetting. Considering these expanding challenges, this paper explores a framework that leverages the benefits of a pre-trained language model along with knowledge distillation in a seq2seq architecture to facilitate translation for low-resource languages, including those not covered by mBART-50. The proposed framework employs a multilingual encoder-based seq2seq model as the foundational architecture and subsequently uses complementary knowledge distillation techniques to mitigate the impact of imbalanced training. Our framework is evaluated on three low-resource Indic languages in four Indic-to-Indic directions, yielding significant BLEU-4 and chrF improvements over baselines. Further, we conduct human evaluation to confirm effectiveness of our approach. Our code is publicly available at https://github.com/raypretam/Two-step-low-res-NMT.","Aniruddha Roy, Pretam Ray, Ayush Maheshwari, Sudeshna Sarkar, Pawan Goyal",2024/7/9,Journal,arXiv preprint arXiv:2407.06538,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:5bg8sr1QxYwC
ArgGen: Prompting text generation models for document-level event-argument aggregation,"Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However, we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work, we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically, we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper, we show the effectiveness of u prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be leveraged in other related multilingual text generation tasks as well: https://github. com/DebanjanaKar/ArgGen.","Debanjana Kar, Sudeshna Sarkar, Pawan Goyal",2022/11,Conference,Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:Br1UauaknNIC
Does Meta-learning Help mBERT for Few-shot Question Generation in a Cross-lingual Transfer Setting for Indic Languages?,"Few-shot Question Generation (QG) is an important and challenging problem in the Natural Language Generation (NLG) domain. Multilingual BERT (mBERT) has been successfully used in various Natural Language Understanding (NLU) applications. However, the question of how to utilize mBERT for few-shot QG, possibly with cross-lingual transfer, remains. In this paper, we try to explore how mBERT performs in few-shot QG (cross-lingual transfer) and also whether applying meta-learning on mBERT further improves the results. In our setting, we consider mBERT as the base model and fine-tune it using a seq-to-seq language modeling framework in a cross-lingual setting. Further, we apply the model agnostic meta-learning approach to our base model. We evaluate our model for two low-resource Indian languages, Bengali and Telugu, using the TyDi QA dataset. The proposed approach consistently improves the performance of the base model in few-shot settings and even works better than some heavily parameterized models. Human evaluation also confirms the effectiveness of our approach.","Aniruddha Roy, Rupak Kumar Thakur, Isha Sharma, Ashim Gupta, Amrith Krishna, Sudeshna Sarkar, Pawan Goyal",2022/10,Conference,Proceedings of the 29th International Conference on Computational Linguistics,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:CB2v5VPnA5kC
Systematic Investigation of Strategies Tailored for Low-Resource Settings for Low-Resource Dependency Parsing,"In this work, we focus on low-resource dependency parsing for multiple languages. Several strategies are tailored to enhance performance in low-resource scenarios. While these are well-known to the community, it is not trivial to select the best-performing combination of these strategies for a low-resource language that we are interested in, and not much attention has been given to measuring the efficacy of these strategies. We experiment with 5 low-resource strategies for our ensembled approach on 7 Universal Dependency (UD) low-resource languages. Our exhaustive experimentation on these languages supports the effective improvements for languages not covered in pretrained models. We show a successful application of the ensembled system on a truly low-resource language Sanskrit. The code and data are available at: https://github.com/Jivnesh/SanDP","Jivnesh Sandhan, Laxmidhar Behera, Pawan Goyal",2022/1/27,Journal,arXiv preprint arXiv:2201.11374,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:jFemdcug13IC
ArgFuse: A Weakly-Supervised Framework for Document-Level Event Argument Aggregation,"Most of the existing information extraction frameworks (Wadden et al., 2019; Veysehet al., 2020) focus on sentence-level tasks and are hardly able to capture the consolidated information from a given document. In our endeavour to generate precise document-level information frames from lengthy textual records, we introduce the task of Information Aggregation or Argument Aggregation. More specifically, our aim is to filter irrelevant and redundant argument mentions that were extracted at a sentence level and render a document level information frame. Majority of the existing works have been observed to resolve related tasks of document-level event argument extraction (Yang et al., 2018a; Zheng et al., 2019a) and salient entity identification (Jain et al.,2020) using supervised techniques. To remove dependency from large amounts of labelled data, we explore the task of information aggregation using weakly-supervised techniques. In particular, we present an extractive algorithm with multiple sieves which adopts active learning strategies to work efficiently in low-resource settings. For this task, we have annotated our own test dataset comprising of 131 document information frames and have released the code and dataset to further research prospects in this new domain. To the best of our knowledge, we are the first to establish baseline results for this task in English. Our data and code are publicly available at https://github.com/DebanjanaKar/ArgFuse.","Debanjana Kar, Sudeshna Sarkar, Pawan Goyal",2021/6/21,Journal,arXiv preprint arXiv:2106.10862,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:Ak0FvsSvgGUC
CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages,"Neural dependency parsing has achieved remarkable performance for low resource morphologically rich languages. It has also been well-studied that morphologically rich languages exhibit relatively free word order. This prompts a fundamental investigation: Is there a way to enhance dependency parsing performance, making the model robust to word order variations utilizing the relatively free word order nature of morphologically rich languages? In this work, we examine the robustness of graph-based parsing architectures on 7 relatively free word order languages. We focus on scrutinizing essential modifications such as data augmentation and the removal of position encoding required to adapt these architectures accordingly. To this end, we propose a contrastive self-supervised learning method to make the model robust to word order variations. Furthermore, our proposed modification demonstrates a substantial average gain of 3.03/2.95 points in 7 relatively free word order languages, as measured by the UAS/LAS Score metric when compared to the best performing baseline.","Pretam Ray, Jivnesh Sandhan, Amrith Krishna, Pawan Goyal",2024/10/9,Journal,arXiv preprint arXiv:2410.06944,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:DkZNVXde3BIC
Biomedical Relation Classification by single and multiple source domain adaptation,"Relation classification is crucial for inferring semantic relatedness between entities in a piece of text. These systems can be trained given labelled data. However, relation classification is very domain-specific and it takes a lot of effort to label data for a new domain. In this paper, we explore domain adaptation techniques for this task. While past works have focused on single source domain adaptation for bio-medical relation classification, we classify relations in an unlabeled target domain by transferring useful knowledge from one or more related source domains. Our experiments with the model have shown to improve state-of-the-art F1 score on 3 benchmark biomedical corpora for single domain and on 2 out of 3 for multi-domain scenarios. When used with contextualized embeddings, there is further boost in performance outperforming neural-network based domain adaptation baselines for both the cases.","Sinchani Chakraborty, Sudeshna Sarkar, Pawan Goyal, Mahanandeeshwar Gattu",2019/11,Conference,Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019),https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:oNZyr7d5Mn4C
Challenges of coreference resolution in biomedical text and experimenting with the biological entities,"The evaluation of document processing activities such as information retrieval can be improved with the help of coreference resolution. However, the resolution process incurs challenges in biomedical domain unlike the same in general English domain. In this paper, we investigate the challenges, reporting some domain-specific solutions to deal with the challenges. Moreover, we have provided some insight upon how an ensemble of rule-based and machine learning based approach can be applied on the existing biomedical baseline coreference resolver to show improved results on the publicly available BioNLP 2011 ST dataset for protein coreference resolution, beating the state-of-the-art baseline system BioSCores by 0.6% F-Score. We also performed an analysis of the important features to obtain good performance.","Ishani Mondal, Sudeshna Sarkar, Pawan Goyal",2018,,,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:epqYDVWIO7EC
A distributed platform for Sanskrit processing,"Sanskrit, the classical language of India, presents specific challenges for computational linguistics: exact phonetic transcription in writing that obscures word boundaries, rich morphology and an enormous corpus, among others. Recent international cooperation has developed innovative solutions to these problems and significant resources for linguistic research. Solutions include efficient segmenting and tagging algorithms and dependency parsers based on constraint programming. The integration of lexical resources, text archives and linguistic software is achieved by distributed interoperable Web services. Resources include a morphological tagger and tagged corpus.","Pawan Goyal, Gérard Huet, Amba Kulkarni, Peter Scharf, Ralph Bunker",2012/12,Conference,Proceedings of COLING 2012,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:tOudhMTPpwUC
Design and analysis of a lean interface for sanskrit corpus annotation,"We describe an innovative computer interface designed to assist annotators in the efficient selection of segmentation solutions for proper tagging of Sanskrit corpora. The proposed solution uses a compact representation of the shared forest of all segmentations. The main idea is to represent the union of all segmentations, abstracting from the sandhi rules used, and aligning with the input sentence. We show that this representation provides an exponential saving, in both space and time.","Pawan Goyal, Gérard Huet",2016/10/18,Journal,Journal of Language Modelling,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:Y5dfb0dijaUC
Analysis of Sanskrit text: Parsing and semantic relations,"In this paper, we are presenting our work towards building a dependency parser for Sanskrit language that uses deterministic finite automata (DFA) for morphological analysis and’utsarga apavaada’approach for relation analysis. A computational grammar based on the framework of Panini is being developed. A linguistic generalization for Verbal and Nominal database has been made and declensions are given the form of DFA. Verbal database for all the class of verbs have been completed for this part. Given a Sanskrit text, the parser identifies the root words and gives the dependency relations based on semantic constraints. The proposed Sanskrit parser is able to create semantic nets for many classes of Sanskrit paragraphs (a Ú). The parser is taking care of both external and internal sandhi in the Sanskrit words.","Pawan Goyal, Vipul Arora, Laxmidhar Behera",2007/10/29,Book,International Sanskrit Computational Linguistics Symposium,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:2osOgNQ5qMEC
Completeness analysis of a Sanskrit reader,We analyse in this paper differences of linguistic treatment of Sanskrit in the Sanskrit Heritage platform1 and in the Paninian grammatical tradition.,"Pawan Goyal, Gérard Huet",2013,Journal,"Proceedings, 5th International Symposium on Sanskrit Computational Linguistics. DK Printworld (P) Ltd",https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:u-x6o8ySG0sC
Translation divergence in English-Sanskrit-Hindi language pairs,"The development of a machine translation system needs that we identify the patterns of divergence between two languages. Though a number of MT developers have given attention to this problem, it is difficult to derive general strategies which can be used for any language pair. Therefore, further exploration is always needed to identify different sources of translation divergence in different pairs of translation languages. In this paper, we discuss translation pattern between English-Sanskrit and Hindi-Sanskrit of various constructions to identify the divergence in English-Sanskrit-Hindi language pairs. This will enable us to come up with strategies to handle these situations and coming up with correct translation. The base has been the classification of translation divergence presented by Dorr [Dorr, 1994].","Pawan Goyal, R Mahesh K Sinha",2009/1/15,Book,International Sanskrit Computational Linguistics Symposium,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:d1gkVwhDpl0C
Design of a lean interface for Sanskrit corpus annotation,"We describe an innovative computer interface designed for assisting annotators in the efficient selection of segmentation solutions for proper tagging of Sanskrit corpus. The proposed solution uses a compact representation of the shared forest of all segmentations. The main idea is to represent the union of all segmentations, abstracting on the sandhi rules used, and aligning on the input sentence. We show that this representation allows an exponential saving, both in space and time. This interface has been implemented, and has been applied to the annotation of the Sanskrit Library corpus.","Gérard Huet, Pawan Goyal",2013,Journal,Proceedings of ICON,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:YsMSGLbcyi4C
Meter identification of sanskrit verse,"A significant portion of Sanskrit literature composed over more than three millennia beginning with the Vedic hymns is composed in metrical verse. Discussion of particular types of Sanskrit meter appears even in the oldest extant Vedic text, the Rgveda, and the science of meter documenting various poetic meters is mentioned in the oldest lists of disciplines. Based on an analysis of the standard classical works of the science of poetics, Velankar (1949) compiled an exhaustive list of more than six hundred different meters which was included by Apte, Gode, and Karve (1957–1959) as an appendix. The present paper presents Web-based software that analyzes Sanskrit metrical patterns and identifies meters. While using a precise phonetic encoding it yet allows numerous input methods and accepts either accented or unaccented text. The software has been successfully tested on a database of 1031 verses in the Pañcakhyanaka, including 291 verses in 23 different types of meters besides Anus. t. ubh. The software should be widely useful to Sanskrit students and scholars, especially those who focus on poetics.","Keshav S Melnad, Pawan Goyal, Peter Scharf",2013/4/9,Journal,"The Sanskrit Library, USA",https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:p2g8aNsByqUC
A study towards design of an English to Sanskrit machine translation system,"We are experimenting to examine how AnglaBharati system designed to translate English to Indian languages could be adapted for translation to Sanskrit. The main contribution of our work is demonstration of machine translation of English to Sanskrit for simple sentences based on PLIL generated by AnglaBharati and Aṣṭādhyāyī rules. Presently our translation system caters to affirmative, negative, interrogative, imperative, active and passive voice sentences. In our study, we have selected a set of nouns and verbs that represent different semantic categories besides a few adverbs and adjectives. We anticipate using a number of Sanskrit resources on Aṣṭādhyāyī and morphological synthesis [2].","Pawan Goyal, R Mahesh K Sinha",2007/10/29,Book,International Sanskrit Computational Linguistics Symposium,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:u5HHmVD_uO8C
Converting Phrase Structures to Dependency Structures in Sanskrit,"Two annotations schemes for presenting the parsed structures are prevalent viz. the constituency structure and the dependency structure. While the constituency trees mark the relations due to positions, the dependency relations mark the semantic dependencies. Free word order languages like Sanskrit pose more problems for constituency parses since the elements within a phrase are dislocated. In this work, we show how the enriched constituency tree with the information of displacement can help construct the unlabelled dependency tree automatically.","Pawan Goyal, Amba Kulkarni",2014,Conference,Coling 2014,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:0EnyYjriUFMC
"Voice, preverb, and transitivity restrictions in sanskrit verb use","In the third pada of the As. t. adhy ayı Pan. ini states rules that account for the occurrence of verbs in the active, middle, and passive voices under semantic and coocurrence conditions. These rules include specific conditions that specify not only which verbs occur in the active or middle voice but also in which meanings they do so, with which preverbs they do so, and whether they do so when transitive or intransitive. Some of the information in these rules is included in comprehensive bilingual lexical and grammatical reference sources such as Böhtlingk and Roth (1855–1875), and some reference sources such as Oberlies (2003) provide complementary information for specific genres and periods. Yet common reference works such as Whitney (1885, 1945) do not include these specific conditions, and the information has yet to be used in any systematic way to analyze Sanskrit syntax. The authors collect and …","Peter Scharf, Pawan Goyal, Anuja Ajotikar, Tanuja Ajotikar",2015/2/16,Journal,"Sanskrit Syntax, Selected papers presented at the seminar on sanskrit syntax and discouse structures",https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:u9iWguZQMMsC
Proceedings of the 6th International Sanskrit Computational Linguistics Symposium,"Welcome to the 6th edition of the International Sanskrit Computational Linguistics Symposium (6th ISCLS) at IIT Kharagpur, West Bengal, India. The aim of ISCLS is to bring together researchers interested in any aspects of Sanskrit Computational Linguistics. Full papers were invited on original and unpublished research on various aspects of Computational Linguistics and Digital Humanities related to Sanskrit (Classical and Vedic), Prakrit, Pali, Buddhist Hybrid Sanskrit, etc. 13 contributions were accepted, and the final versions, after incorporating the reviewers’ comments constitute the proceedings. We would like to thank the Program Committee for the 6th ISCLS for their reviewing efforts:",Pawan Goyal,2019/10,Conference,Proceedings of the 6th International Sanskrit Computational Linguistics Symposium,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:kz9GbA2Ns4gC
A graph-based framework for structured prediction tasks in Sanskrit,"We propose a framework using energy-based models for multiple structured prediction tasks in Sanskrit. Ours is an arc-factored model, similar to the graph-based parsing approaches, and we consider the tasks of word segmentation, morphological parsing, dependency parsing, syntactic linearization, and prosodification, a “prosody-level” task we introduce in this work. Ours is a search-based structured prediction framework, which expects a graph as input, where relevant linguistic information is encoded in the nodes, and the edges are then used to indicate the association between these nodes. Typically, the state-of-the-art models for morphosyntactic tasks in morphologically rich languages still rely on hand-crafted features for their performance. But here, we automate the learning of the feature function. The feature function so learned, along with the search space we construct, encode relevant linguistic information for the tasks we consider. This enables us to substantially reduce the training data requirements to as low as 10%, as compared to the data requirements for the neural state-of-the-art models. Our experiments in Czech and Sanskrit show the language-agnostic nature of the framework, where we train highly competitive models for both the languages. Moreover, our framework enables us to incorporate language-specific constraints to prune the search space and to filter the candidates during inference. We obtain significant improvements in morphosyntactic tasks for Sanskrit by incorporating language-specific constraints into the model. In all the tasks we discuss for Sanskrit, we either achieve state-of-the-art results or ours is the only data-driven solution for those tasks.","Amrith Krishna, Bishal Santra, Ashim Gupta, Pavankumar Satuluri, Pawan Goyal",2021/2/1,Journal,Computational Linguistics,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:yMeIxYmEMEAC
Automatic speech recognition in Sanskrit: A new speech corpus and modelling insights,"Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the various linguistic peculiarities present in the language. The Sanskrit language is lexically productive, undergoes euphonic assimilation of phones at the word boundaries and exhibits variations in spelling conventions and in pronunciations. In this work, we propose the first large scale study of automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR dataset for Sanskrit, which faithfully captures several of the linguistic characteristics expressed by the language. We investigate the role of different acoustic model and language model units in ASR systems for Sanskrit. We also propose a new modelling unit, inspired by the syllable level unit selection, that captures character sequences from one vowel in the word to the next vowel. We also highlight the importance of choosing graphemic representations for Sanskrit and show the impact of this choice on word error rates (WER). Finally, we extend these insights from Sanskrit ASR for building ASR systems in two other Indic languages, Gujarati and Telugu. For both these languages, our experimental results show that the use of phonetic based graphemic representations in ASR results in performance improvements as compared to ASR systems that use native scripts.","Devaraja Adiga, Rishabh Kumar, Amrith Krishna, Preethi Jyothi, Ganesh Ramakrishnan, Pawan Goyal",2021/6/2,Journal,arXiv preprint arXiv:2106.05852,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:vDZJ-YLwNdEC
A dataset for Sanskrit word segmentation,"The last decade saw a surge in digitisation efforts for ancient manuscripts in Sanskrit. Due to various linguistic peculiarities inherent to the language, even the preliminary tasks such as word segmentation are non-trivial in Sanskrit. Elegant models for Word Segmentation in Sanskrit are indispensable for further syntactic and semantic processing of the manuscripts. Current works in word segmentation for Sanskrit, though commendable in their novelty, often have variations in their objective and evaluation criteria. In this work, we set the record straight. We formally define the objectives and the requirements for the word segmentation task. In order to encourage research in the field and to alleviate the time and effort required in pre-processing, we release a dataset of 115,000 sentences for word segmentation. For each sentence in the dataset we include the input character sequence, ground truth segmentation, and additionally lexical and morphological information about all the phonetically possible segments for the given sentence. In this work, we also discuss the linguistic considerations made while generating the candidate space of the possible segments.","Amrith Krishna, Pavankumar Satuluri, Pawan Goyal",2017/8,Conference,"Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:eq2jaN3J8jMC
Building a word segmenter for Sanskrit overnight,"There is an abundance of digitised texts available in Sanskrit. However, the word segmentation task in such texts are challenging due to the issue of 'Sandhi'. In Sandhi, words in a sentence often fuse together to form a single chunk of text, where the word delimiter vanishes and sounds at the word boundaries undergo transformations, which is also reflected in the written text. Here, we propose an approach that uses a deep sequence to sequence (seq2seq) model that takes only the sandhied string as the input and predicts the unsandhied string. The state of the art models are linguistically involved and have external dependencies for the lexical and morphological analysis of the input. Our model can be trained ""overnight"" and be used for production. In spite of the knowledge lean approach, our system preforms better than the current state of the art by gaining a percentage increase of 16.79 % than the current state of the art.","Vikas Reddy, Amrith Krishna, Vishnu Dutt Sharma, Prateek Gupta, Pawan Goyal",2018/2/17,Journal,arXiv preprint arXiv:1802.06185,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:fEOibwPWpKIC
Compound type identification in sanskrit: what roles do the corpus and grammar play?,"We propose a classification framework for semantic type identification of compounds in Sanskrit. We broadly classify the compounds into four different classes namely, Avyayībhāva, Tatpuruṣa, Bahuvrīhi and Dvandva. Our classification is based on the traditional classification system followed by the ancient grammar treatise Adṣṭādhyāyī, proposed by Pāṇini 25 centuries back. We construct an elaborate features space for our system by combining conditional rules from the grammar Adṣṭādhyāyī, semantic relations between the compound components from a lexical database Amarakoṣa and linguistic structures from the data using Adaptor Grammars. Our in-depth analysis of the feature space highlight inadequacy of Adṣṭādhyāyī, a generative grammar, in classifying the data samples. Our experimental results validate the effectiveness of using lexical databases as suggested by Amba Kulkarni and Anil Kumar, and put forward a new research direction by introducing linguistic patterns obtained from Adaptor grammars for effective identification of compound type. We utilise an ensemble based approach, specifically designed for handling skewed datasets and we% and Experimenting with various classification methods, we achieve an overall accuracy of 0.77 using random forest classifiers.","Amrith Krishna, Pavankumar Satuluri, Shubham Sharma, Apurv Kumar, Pawan Goyal",2016/12,Conference,Proceedings of the 6th Workshop on South and Southeast Asian Natural Language Processing (WSSANLP2016),https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:JoZmwDi-zQgC
Word segmentation in sanskrit using path constrained random walks,"In Sanskrit, the phonemes at the word boundaries undergo changes to form new phonemes through a process called as sandhi. A fused sentence can be segmented into multiple possible segmentations. We propose a word segmentation approach that predicts the most semantically valid segmentation for a given sentence. We treat the problem as a query expansion problem and use the path-constrained random walks framework to predict the correct segments.","Amrith Krishna, Bishal Santra, Pavankumar Satuluri, Sasi Prasanth Bandaru, Bhumi Faldu, Yajuvendra Singh, Pawan Goyal",2016/12,Conference,"Proceedings of COLING 2016, the 26th international conference on computational linguistics: Technical papers",https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:9vf0nzSNQJEC
Evaluating neural morphological taggers for Sanskrit,"Neural sequence labelling approaches have achieved state of the art results in morphological tagging. We evaluate the efficacy of four standard sequence labelling models on Sanskrit, a morphologically rich, fusional Indian language. As its label space can theoretically contain more than 40,000 labels, systems that explicitly model the internal structure of a label are more suited for the task, because of their ability to generalise to labels not seen during training. We find that although some neural models perform better than others, one of the common causes for error for all of these models is mispredictions due to syncretism.","Ashim Gupta, Amrith Krishna, Pawan Goyal, Oliver Hellwig",2020/5/21,Journal,arXiv preprint arXiv:2005.10893,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:XoXfffV-tXoC
Free as in free word order: An energy based model for word segmentation and morphological tagging in Sanskrit,"The configurational information in sentences of a free word order language such as Sanskrit is of limited use. Thus, the context of the entire sentence will be desirable even for basic processing tasks such as word segmentation. We propose a structured prediction framework that jointly solves the word segmentation and morphological tagging tasks in Sanskrit. We build an energy based model where we adopt approaches generally employed in graph based parsing techniques (McDonald et al., 2005a; Carreras, 2007). Our model outperforms the state of the art with an F-Score of 96.92 (percentage improvement of 7.06%) while using less than one-tenth of the task-specific training data. We find that the use of a graph based ap- proach instead of a traditional lattice-based sequential labelling approach leads to a percentage gain of 12.6% in F-Score for the segmentation task.","Amrith Krishna, Bishal Santra, Sasi Prasanth Bandaru, Gaurav Sahu, Vishnu Dutt Sharma, Pavankumar Satuluri, Pawan Goyal",2018/9/5,Journal,arXiv preprint arXiv:1809.01446,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:uc_IGeMz5qoC
Poetry to prose conversion in Sanskrit as a linearisation task: A case for low-resource languages,"The word ordering in a Sanskrit verse is often not aligned with its corresponding prose order. Conversion of the verse to its corresponding prose helps in better comprehension of the construction. Owing to the resource constraints, we formulate this task as a word ordering (linearisation) task. In doing so, we completely ignore the word arrangement at the verse side. kāvya guru, the approach we propose, essentially consists of a pipeline of two pretraining steps followed by a seq2seq model. The first pretraining step learns task-specific token embeddings from pretrained embeddings. In the next step, we generate multiple possible hypotheses for possible word arrangements of the input% using another pretraining step. We then use them as inputs to a neural seq2seq model for the final prediction. We empirically show that the hypotheses generated by our pretraining step result in predictions that consistently outperform predictions based on the original order in the verse. Overall, kāvya guru outperforms current state of the art models in linearisation for the poetry to prose conversion task in Sanskrit.","Amrith Krishna, Vishnu Dutt Sharma, Bishal Santra, Aishik Chakraborty, Pavankumar Satuluri, Pawan Goyal",2019/7,Conference,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:KUbvn5osdkgC
Upcycle your OCR: Reusing OCRs for post-OCR text correction in Romanised Sanskrit,"We propose a post-OCR text correction approach for digitising texts in Romanised Sanskrit. Owing to the lack of resources our approach uses OCR models trained for other languages written in Roman. Currently, there exists no dataset available for Romanised Sanskrit OCR. So, we bootstrap a dataset of 430 images, scanned in two different settings and their corresponding ground truth. For training, we synthetically generate training images for both the settings. We find that the use of copying mechanism (Gu et al., 2016) yields a percentage increase of 7.69 in Character Recognition Rate (CRR) than the current state of the art model in solving monotone sequence-to-sequence tasks (Schnober et al., 2016). We find that our system is robust in combating OCR-prone errors, as it obtains a CRR of 87.01% from an OCR output with CRR of 35.76% for one of the dataset settings. A human judgment survey performed on the models shows that our proposed model results in predictions which are faster to comprehend and faster to improve for a human than the other systems.","Amrith Krishna, Bodhisattwa Prasad Majumder, Rajesh Shreedhar Bhat, Pawan Goyal",2018/9/6,Journal,arXiv preprint arXiv:1809.02147,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:EkHepimYqZsC
Keep it surprisingly simple: A simple first order graph based parsing model for joint morphosyntactic parsing in Sanskrit,"Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures. We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit. Here, we extend the Energy based model framework (Krishna et al., 2020), proposed for several structured prediction tasks in Sanskrit, in 2 simple yet significant ways. First, the framework’s default input graph generation method is modified to generate a multigraph, which enables the use of an exact search inference. Second, we prune the input search space using a linguistically motivated approach, rooted in the traditional grammatical analysis of Sanskrit. Our experiments show that the morphological parsing from our joint model outperforms standalone morphological parsers. We report state of the art results in morphological parsing, and in dependency parsing, both in standalone (with gold morphological tags) and joint morphosyntactic parsing setting.","Amrith Krishna, Ashim Gupta, Deepak Garasangi, Pavankumar Satuluri, Pawan Goyal",2020/11,Conference,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:fFSKOagxvKUC
TransLIST: A transformer-based linguistically informed Sanskrit tokenizer,"Sanskrit Word Segmentation (SWS) is essential in making digitized texts available and in deploying downstream tasks. It is, however, non-trivial because of the sandhi phenomenon that modifies the characters at the word boundaries, and needs special treatment. Existing lexicon driven approaches for SWS make use of Sanskrit Heritage Reader, a lexicon-driven shallow parser, to generate the complete candidate solution space, over which various methods are applied to produce the most valid solution. However, these approaches fail while encountering out-of-vocabulary tokens. On the other hand, purely engineering methods for SWS have made use of recent advances in deep learning, but cannot make use of the latent word information on availability. To mitigate the shortcomings of both families of approaches, we propose Transformer based Linguistically Informed Sanskrit Tokenizer (TransLIST) consisting of (1) a module that encodes the character input along with latent-word information, which takes into account the sandhi phenomenon specific to SWS and is apt to work with partial or no candidate solutions, (2) a novel soft-masked attention to prioritize potential candidate words and (3) a novel path ranking algorithm to rectify the corrupted predictions. Experiments on the benchmark datasets for SWS show that TransLIST outperforms the current state-of-the-art system by an average 7.2 points absolute gain in terms of perfect match (PM) metric. The codebase and datasets are publicly available at https://github.com/rsingha108/TransLIST","Jivnesh Sandhan, Rathin Singha, Narein Rao, Suvendu Samanta, Laxmidhar Behera, Pawan Goyal",2022/10/21,Journal,arXiv preprint arXiv:2210.11753,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:XUvXOeBm_78C
Revisiting the role of feature engineering for compound type identification in Sanskrit,"We propose an automated approach for semantic class identification of compounds in Sanskrit. It is essential to extract semantic information hidden in compounds for improving overall downstream Natural Language Processing (NLP) applications such as information extraction, question answering, machine translation, and many more. In this work, we systematically investigate the following research question: Can recent advances in neural network outperform traditional hand engineered feature based methods on the semantic level multi-class compound classification task for Sanskrit? Contrary to the previous methods, our method does not require feature engineering. For well-organized analysis, we categorize neural systems based on Multi-Layer Perceptron (MLP), Convolution Neural Network (CNN) and Long Short Term Memory (LSTM) architecture and feed input to the system from one of the possible levels, namely, word level, sub-word level, and character level. Our best system with LSTM architecture and FastText embedding with end-to-end training has shown promising results in terms of F-score (0.73) compared to the state of the art method based on feature engineering (0.74) and outperformed in terms of accuracy (77.68%).","Jivnesh Sandhan, Amrith Krishna, Pawan Goyal, Laxmidhar Behera",2019/10,Conference,Proceedings of the 6th international Sanskrit computational linguistics symposium,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:k8Z6L05lTy4C
Evaluating neural word embeddings for Sanskrit,"Recently, the supervised learning paradigm's surprisingly remarkable performance has garnered considerable attention from Sanskrit Computational Linguists. As a result, the Sanskrit community has put laudable efforts to build task-specific labeled data for various downstream Natural Language Processing (NLP) tasks. The primary component of these approaches comes from representations of word embeddings. Word embedding helps to transfer knowledge learned from readily available unlabelled data for improving task-specific performance in low-resource setting. Last decade, there has been much excitement in the field of digitization of Sanskrit. To effectively use such readily available resources, it is very much essential to perform a systematic study on word embedding approaches for the Sanskrit language. In this work, we investigate the effectiveness of word embeddings. We classify word embeddings in broad categories to facilitate systematic experimentation and evaluate them on four intrinsic tasks. We investigate the efficacy of embeddings approaches (originally proposed for languages other than Sanskrit) for Sanskrit along with various challenges posed by language.","Jivnesh Sandhan, Om Adideva, Digumarthi Komal, Laxmidhar Behera, Pawan Goyal",2021/4/1,Journal,arXiv preprint arXiv:2104.00270,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&pagesize=100&citation_for_view=F14FHsIAAAAJ:nVrZBo8bIpAC
Neural approaches for data driven dependency parsing in Sanskrit,"Data-driven approaches for dependency parsing have been of great interest in Natural Language Processing for the past couple of decades. However, Sanskrit still lacks a robust purely data-driven dependency parser, probably with an exception to Krishna (2019). This can primarily be attributed to the lack of availability of task-specific labelled data and the morphologically rich nature of the language. In this work, we evaluate four different data-driven machine learning models, originally proposed for different languages, and compare their performances on Sanskrit data. We experiment with 2 graph based and 2 transition based parsers. We compare the performance of each of the models in a low-resource setting, with 1,500 sentences for training. Further, since our focus is on the learning power of each of the models, we do not incorporate any Sanskrit specific features explicitly into the models, and rather use the default settings in each of the paper for obtaining the feature functions. In this work, we analyse the performance of the parsers using both an in-domain and an out-of-domain test dataset. We also investigate the impact of word ordering in which the sentences are provided as input to these systems, by parsing verses and their corresponding prose order (anvaya) sentences.","Amrith Krishna, Ashim Gupta, Deepak Garasangi, Jivnesh Sandhan, Pavankumar Satuluri, Pawan Goyal",2020/4/17,Journal,arXiv preprint arXiv:2004.08076,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:q3CdL3IzO_QC
Towards automating the generation of derivative nouns in sanskrit by simulating panini,"About 1115 rules in Astadhyayi from A.4.1.76 to A.5.4.160 deal with generation of derivative nouns, making it one of the largest topical sections in Astadhyayi, called as the Taddhita section owing to the head rule A.4.1.76. This section is a systematic arrangement of rules that enumerates various affixes that are used in the derivation under specific semantic relations. We propose a system that automates the process of generation of derivative nouns as per the rules in Astadhyayi. The proposed system follows a completely object oriented approach, that models each rule as a class of its own and then groups them as rule groups. The rule groups are decided on the basis of selective grouping of rules by virtue of anuvrtti. The grouping of rules results in an inheritance network of rules which is a directed acyclic graph. Every rule group has a head rule and the head rule notifies all the direct member rules of the group about the environment which contains all the details about data entities, participating in the derivation process. The system implements this mechanism using multilevel inheritance and observer design patterns. The system focuses not only on generation of the desired final form, but also on the correctness of sequence of rules applied to make sure that the derivation has taken place in strict adherence to Astadhyayi. The proposed system's design allows to incorporate various conflict resolution methods mentioned in authentic texts and hence the effectiveness of those rules can be validated with the results from the system. We also present cases where we have checked the applicability of the system with the rules which are not specifically applicable to derivation of derivative nouns, in order to see the effectiveness of the proposed schema as a generic system for modeling Astadhyayi.","Amrith Krishna, Pawan Goyal",2015/12/17,Journal,arXiv preprint arXiv:1512.05670,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:5Ul4iDaHHb8C
A novel multi-task learning approach for context-sensitive compound type identification in Sanskrit,"The phenomenon of compounding is ubiquitous in Sanskrit. It serves for achieving brevity in expressing thoughts, while simultaneously enriching the lexical and structural formation of the language. In this work, we focus on the Sanskrit Compound Type Identification (SaCTI) task, where we consider the problem of identifying semantic relations between the components of a compound word. Earlier approaches solely rely on the lexical information obtained from the components and ignore the most crucial contextual and syntactic information useful for SaCTI. However, the SaCTI task is challenging primarily due to the implicitly encoded context-sensitive semantic relation between the compound components. Thus, we propose a novel multi-task learning architecture which incorporates the contextual information and enriches the complementary syntactic information using morphological tagging and dependency parsing as two auxiliary tasks. Experiments on the benchmark datasets for SaCTI show 6.1 points (Accuracy) and 7.7 points (F1-score) absolute gain compared to the state-of-the-art system. Further, our multi-lingual experiments demonstrate the efficacy of the proposed architecture in English and Marathi languages.The code and datasets are publicly available at https://github.com/ashishgupta2598/SaCTI","Jivnesh Sandhan, Ashish Gupta, Hrishikesh Terdalkar, Tushar Sandhan, Suvendu Samanta, Laxmidhar Behera, Pawan Goyal",2022/8/22,Journal,arXiv preprint arXiv:2208.10310,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:7wO8s98CvbsC
Pre-annotation based approach for development of a Sanskrit named entity recognition dataset,"Recent NLP algorithms for Named Entity Recognition require a large amount of annotated resources. Low resourced languages such as Sanskrit lack adequate annotated data and manual annotation is costly as well as time-consuming. A sequence labeling tool that supports manual annotation with automatic suggestions to reduce time and increase annotators' productivity is appealing, especially for low-resourced languages. Most of the tools developed with such capabilities are for resource-rich western European languages and are not tuned to the need of morphologically rich language like Sanskrit. In our work, we deploy an annotation tool for crowd-sourcing a Sanskrit NER dataset annotation.","Sarkar Sujoy, Amrith Krishna, Pawan Goyal",2023/1,Conference,Proceedings of the Computational Sanskrit & Digital Humanities: Selected papers presented at the 18th World Sanskrit Conference,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:hsZV8lGYWTMC
SHR++: An interface for morpho-syntactic annotation of Sanskrit corpora,"We propose a web-based annotation framework, SHR++, for morpho-syntactic annotation of corpora in Sanskrit. SHR++ is designed to generate annotations for the word-segmentation, morphological parsing and dependency analysis tasks in Sanskrit. It incorporates analyses and predictions from various tools designed for processing texts in Sanskrit, and utilise them to ease the cognitive load of the human annotators. Specifically, SHR++ uses Sanskrit Heritage Reader, a lexicon driven shallow parser for enumerating all the phonetically and lexically valid word splits along with their morphological analyses for a given string. This would help the annotators in choosing the solutions, rather than performing the segmentations by themselves. Further, predictions from a word segmentation tool are added as suggestions that can aid the human annotators in their decision making. Our evaluation shows that enabling this segmentation suggestion component reduces the annotation time by 20.15%. SHR++ can be accessed online at http://vidhyut97. pythonanywhere. com/and the codebase, for the independent deployment of the system elsewhere, is hosted at https://github. com/iamdsc/smart-sanskrit-annotator.","Amrith Krishna, Shiv Vidhyut, Dilpreet Chawla, Sruti Sambhavi, Pawan Goyal",2020/5,Conference,Proceedings of the Twelfth Language Resources and Evaluation Conference,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:ruyezt5ZtCIC
Linguistically Informed Post-processing for ASR Error correction in Sanskrit.,"We propose an ASR system for Sanskrit, a lowresource language, that effectively combines subword tokenisation strategies and search space enrichment with linguistic information. More specifically, to address the challenges due to the high degree of out-of-vocabulary entries present in the language, we first use a subword-based language model and acoustic model to generate a search space. The search space, so obtained, is converted into a word-based search space and is further enriched with morphological and lexical information based on a shallow parser. Finally, the transitions in the search space are rescored using a supervised morphological parser proposed for Sanskrit. Our proposed approach currently reports the state-of-the-art results in Sanskrit ASR, with a 7.18 absolute point reduction in WER than the previous state-of-the-art.","Rishabh Kumar, Devaraja Adiga, Rishav Ranjan, Amrith Krishna, Ganesh Ramakrishnan, Pawan Goyal, Preethi Jyothi",2022/1/1,Conference,INTERSPEECH,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:wKETBy42zhYC
A graph based semi-supervised approach for analysis of derivational nouns in Sanskrit,"Derivational nouns are widely used in Sanskrit corpora and represent an important cornerstone of productivity in the language. Currently there exists no analyser that identifies the derivational nouns. We propose a semi supervised approach for identification of derivational nouns in Sanskrit. We not only identify the derivational words, but also link them to their corresponding source words. Our novelty comes in the design of the network structure for the task. The edge weights are featurised based on the phonetic, morphological, syntactic and the semantic similarity shared between the words to be identified. We find that our model is effective for the task, even when we employ a labelled dataset which is only 5% to that of the entire dataset.","Amrith Krishna, Pavankumar Satuluri, Harshavardhan Ponnada, Muneeb Ahmed, Gulab Arora, Kaustubh Hiware, Pawan Goyal",2017/8,Conference,Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for Natural Language Processing,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:uLbwQdceFCQC
Sanskritshala: A neural sanskrit nlp toolkit with web-based interface for pedagogical and annotation purposes,"We present a neural Sanskrit Natural Language Processing (NLP) toolkit named SanskritShala (a school of Sanskrit) to facilitate computational linguistic analyses for several tasks such as word segmentation, morphological tagging, dependency parsing, and compound type identification. Our systems currently report state-of-the-art performance on available benchmark datasets for all tasks. SanskritShala is deployed as a web-based application, which allows a user to get real-time analysis for the given input. It is built with easy-to-use interactive data annotation features that allow annotators to correct the system predictions when it makes mistakes. We publicly release the source codes of the 4 modules included in the toolkit, 7 word embedding models that have been trained on publicly available Sanskrit corpora and multiple annotated datasets such as word similarity, relatedness, categorization, analogy prediction to assess intrinsic properties of word embeddings. So far as we know, this is the first neural-based Sanskrit NLP toolkit that has a web-based interface and a number of NLP modules. We are sure that the people who are willing to work with Sanskrit will find it useful for pedagogical and annotative purposes. SanskritShala is available at: https://cnerg.iitkgp.ac.in/sanskritshala. The demo video of our platform can be accessed at: https://youtu.be/x0X31Y9k0mw4.","Jivnesh Sandhan, Anshul Agarwal, Laxmidhar Behera, Tushar Sandhan, Pawan Goyal",2023/2/19,Journal,arXiv preprint arXiv:2302.09527,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:rHJHxKgnXwkC
Depnecti: Dependency-based nested compound type identification for sanskrit,"Multi-component compounding is a prevalent phenomenon in Sanskrit, and understanding the implicit structure of a compound's components is crucial for deciphering its meaning. Earlier approaches in Sanskrit have focused on binary compounds and neglected the multi-component compound setting. This work introduces the novel task of nested compound type identification (NeCTI), which aims to identify nested spans of a multi-component compound and decode the implicit semantic relations between them. To the best of our knowledge, this is the first attempt in the field of lexical semantics to propose this task. We present 2 newly annotated datasets including an out-of-domain dataset for this task. We also benchmark these datasets by exploring the efficacy of the standard problem formulations such as nested named entity recognition, constituency parsing and seq2seq, etc. We present a novel framework named DepNeCTI: Dependency-based Nested Compound Type Identifier that surpasses the performance of the best baseline with an average absolute improvement of 13.1 points F1-score in terms of Labeled Span Score (LSS) and a 5-fold enhancement in inference efficiency. In line with the previous findings in the binary Sanskrit compound identification task, context provides benefits for the NeCTI task. The codebase and datasets are publicly available at: https://github.com/yaswanth-iitkgp/DepNeCTI","Jivnesh Sandhan, Yaswanth Narsupalli, Sreevatsa Muppirala, Sriram Krishnan, Pavankumar Satuluri, Amba Kulkarni, Pawan Goyal",2023/10/14,Journal,arXiv preprint arXiv:2310.09501,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:mKu_rENv82IC
An ‘Ekalavya’Approach to Learning Context Free Grammar Rules for Sanskrit Using Adaptor Grammar,"This work presents the use of Adaptor Grammar, a nonparametric Bayesian approach for learning (Probabilistic) Context-Free Grammar productions from data. In Adaptor Grammar, we provide the set of non-terminals followed by a skeletal grammar that establishes the relations between the non-terminals in the grammar. The productions and the associated probability for the productions are automatically learnt by the system from the usages of words or sentences, ie, the dataset. This facilitates the encoding of prior linguistic knowledge through the skeletal grammar and yet the tiresome task of finding the productions is delegated to the system. The system completely learns the grammar structure by observing the data. We call this approach the ‘Ekalavya’approach. In this work, we discuss the effect of using Adaptor grammars for Sanskrit at word-level supervised tasks such as compound type identification and also in identifying the source and derived words from corpora for derivational nouns. In both of the works, we show the use of sub-word patterns learned using Adaptor grammar as effective features for their corresponding supervised tasks. We also present our novel approach of using Adaptor Grammars for handling Structured Prediction tasks in Sanskrit. We present the preliminary results for the word reordering task in Sanskrit. We also outline our plan for the use of Adaptor grammars for Dependency Parsing and Poetry to Prose Conversion tasks.","Amrith Krishna, Bodhisattwa Prasad Majumder, Anil Kumar Boga, Pawan Goyal",2018,Journal,Computational Sanskrit & Digital Humanities,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:4MWp96NkSFoC
Linguistically informed automatic speech recognition in sanskrit,"The field of Automatic Speech Recognition (ASR) for Sanskrit is marked by distinctive challenges, primarily due to the language’s intricate linguistic and morphological characteristics. Recognizing the burgeoning interest in this domain, we present the ‘Vāksañcayah’ speech corpus, a comprehensive collection that captures the linguistic depth and complexities of Sanskrit. Building upon our prior work, which focused on various acoustic model (AM) and language model (LM) units, we present an enhanced ASR system. This system integrates innovative subword tokenization methods and enriches the search space with linguistic insights. Addressing the issue of high out-of-vocabulary (OOV) rates and the prevalence of infrequently used words in Sanskrit, we employed a subword-based language model. Our approach mitigates these challenges and facilitates the generation of a subword-based search space. While effective in numerous scenarios, this model encounters limitations regarding long-range dependencies and semantic context comprehension. To counter these limitations, we leveraged Sanskrit’s rich morphological framework, thus achieving a more holistic understanding. The subword-based search space is subsequently transformed into a word-based format and augmented with morphological and lexical data, derived from a lexically driven shallow parser. Enhancing this further, we rescore transitions within this enriched space using a supervised morphological parser specifically designed for Sanskrit. Our proposed methodology is currently acclaimed as the most advanced in the realm of Sanskrit ASR, achieving a Word Error Rate (WER) of 12.54 and an improvement of 3.77 absolute points over the previous best. Additionally, we annotated 500 utterances with detailed morphological data and their corresponding lemmas, providing a basis for extensive linguistic analysis.","Rishabh Kumar, Devaraja Adiga, Rishav Ranjan, Amrith Krishna, Ganesh Ramakrishnan, Pawan Goyal, Preethi Jyothi",2024,Journal,Available at SSRN 4790493,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:3bvyWxjaHKcC
Aesthetics of Sanskrit Poetry from the Perspective of Computational Linguistics: A Case Study Analysis on Siksastaka,"Sanskrit poetry has played a significant role in shaping the literary and cultural landscape of the Indian subcontinent for centuries. However, not much attention has been devoted to uncovering the hidden beauty of Sanskrit poetry in computational linguistics. This article explores the intersection of Sanskrit poetry and computational linguistics by proposing a roadmap of an interpretable framework to analyze and classify the qualities and characteristics of fine Sanskrit poetry. We discuss the rich tradition of Sanskrit poetry and the significance of computational linguistics in automatically identifying the characteristics of fine poetry. The proposed framework involves a human-in-the-loop approach that combines deterministic aspects delegated to machines and deep semantics left to human experts. We provide a deep analysis of Siksastaka, a Sanskrit poem, from the perspective of 6 prominent kavyashastra schools, to illustrate the proposed framework. Additionally, we provide compound, dependency, anvaya (prose order linearised form), meter, rasa (mood), alankar (figure of speech), and riti (writing style) annotations for Siksastaka and a web application to illustrate the poem's analysis and annotations. Our key contributions include the proposed framework, the analysis of Siksastaka, the annotations and the web application for future research. Link for interactive analysis: https://sanskritshala.github.io/shikshastakam/","Rishabh Kumar, Devaraja Adiga, Rishav Ranjan, Amrith Krishna, Ganesh Ramakrishnan, Pawan Goyal, Preethi Jyothi",2024,Journal,arXiv preprint arXiv:2308.07081,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=100&pagesize=100&citation_for_view=F14FHsIAAAAJ:NDuN12AVoxsC
Supplementary Material for Keep It Surprisingly Simple: A Simple First Order Graph Based Parsing Model for Joint Morphosyntactic Parsing in Sanskrit,"Cconsider a 14 word verse,“Sriyah. patih. srimati sasitu m jagat jagannivasah. vasudevasadmani vasan dadarsa avatarantam ambarat hiran. yagarbhangabhuvam munim harih.” 1, from the literary work ‘Sisupalavadha’. Here, the sequence is in its segmented form, and yet it would result in 23,040 different possible sentences owing to syncretism and homonymy. 2 The analysis for the given sequence is shown in Figure 1. Further, each of the sentence would result in nn− 2 unlabelled spanning trees if we consider an unpruned complete graph as the input. Here n is the number of tokens in the sentence, ie 14 for the given sentence. This will result in a prohibitively large space of possible spanning trees, if we consider the possible spanning trees for all the 23,040 possible sentences. However, with our linguistic pruning we could restrict the number of total possible spanning trees to be just about 102,360 trees (combined count for all the 23,040 sentences). As discussed in the main paper, an input multigraph is formed from the morphological analysis. Spanning trees are enumerated from the input multigraph and for each spanning tree, we evaluate its validity as a candidate dependency tree. Based on the edge selected from the input multigraph, every node in the tree will have a specific morphological tag. Further, we will assume the edges to be labelled with dependency relations, but only with those which are applicable as per the morphological tags of the nodes in the edge. If the tree evaluates to be a valid candidate, then its edges are retained in the multigraph. However, the label information","Amrith Krishna, Ashim Gupta, Deepak Garasangi, Pavankumar Satuluri, Pawan Goyal",,,,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=200&pagesize=100&citation_for_view=F14FHsIAAAAJ:KbBQZpvPDL4C
Supplementary Materials for “Free as in Free Word Order: An Energy Based Model for Word Segmentation and Morphological Tagging in Sanskrit”,"Sandhi: The phonemes at the word boundaries are often merged using phonetic transformations called as “sandhi”.“sandhi” is primarily an outcome of the euphonic assimilation in speech, that gets reflected in writing as well (Goyal and Huet, 2016). The proximity between phonemes is the sole criteria for applying sandhi The analysis of a construction with sandhi in it can often lead to a word splits with possible gaps and overlaps between them. In Figure 1 b and 1 c we can find the word splits have overlaps between them. This happens as the multiple phonemes join together to form a single phoneme at the time of sandhi. Rarely, instance like Figure 2 d occurs. Here the sandhi led to generation of an additional phoneme ‘n’it it. Figure 1 a is a case where the word splits do not have overlap with the sandhied version. At the same time, the phonetic change in the sandhied version can be observed.","Amrith Krishna, Bishal Santra, Sasi Prasanth Bandaru, Gaurav Sahu, Vishnu Dutt Sharma, Pavankumar Satuluri, Pawan Goyal",,,,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=F14FHsIAAAAJ&cstart=200&pagesize=100&citation_for_view=F14FHsIAAAAJ:ipzZ9siozwsC
Mahānāma: A Unique Testbed for Literary Entity Discovery and Linking,"High lexical variation, ambiguous references, and long-range dependencies make entity resolution in literary texts particularly challenging. We present Mahānāma, the first large-scale dataset for end-to-end Entity Discovery and Linking (EDL) in Sanskrit, a morphologically rich and under-resourced language. Derived from the Mahābhārata, the world's longest epic, the dataset comprises over 109K named entity mentions mapped to 5.5K unique entities, and is aligned with an English knowledge base to support cross-lingual linking. The complex narrative structure of Mahānāma, coupled with extensive name variation and ambiguity, poses significant challenges to resolution systems. Our evaluation reveals that current coreference and entity linking models struggle when evaluated on the global context of the test set. These results highlight the limitations of current approaches in resolving entities within such complex discourse. Mahānāma thus provides a unique benchmark for advancing entity resolution, especially in literary domains.","Sujoy Sarkar, Gourav Sarkar, Manoj Balaji Jagadeeshan, Jivnesh Sandhan, Amrith Krishna, Pawan Goyal",2025/9,Journal,arXiv: 2509.19844,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=SSIMTwgAAAAJ&citation_for_view=SSIMTwgAAAAJ:qjMakFHDy7sC
From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses,"Recent advances in large language models (LLMs) have significantly improved natural language generation, including creative tasks like poetry composition. However, most progress remains concentrated in high-resource languages. This raises an important question: Can LLMs be adapted for structured poetic generation in a low-resource, morphologically rich language such as Sanskrit? In this work, we introduce a dataset designed for translating English prose into structured Sanskrit verse, with strict adherence to classical metrical patterns, particularly the Anushtub meter. We evaluate a range of generative models-both open-source and proprietary-under multiple settings. Specifically, we explore constrained decoding strategies and instruction-based fine-tuning tailored to metrical and semantic fidelity. Our decoding approach achieves over 99% accuracy in producing syntactically valid poetic forms, substantially outperforming general-purpose models in meter conformity. Meanwhile, instruction-tuned variants show improved alignment with source meaning and poetic style, as supported by human assessments, albeit with marginal trade-offs in metrical precision.","Manoj Balaji Jagadeeshan, Samarth Bhatia, Pretam Ray, Harshul Raj Surana, Akhil Rajeev P, Priya Mishra, Annarao Kulkarni, Ganesh Ramakrishnan, Prathosh AP, Pawan Goyal",2025/6,Journal,arXiv: 2506.00815,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=5eQgpf8AAAAJ&citation_for_view=5eQgpf8AAAAJ:qjMakFHDy7sC
Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry,"Sanskrit, an ancient language with a rich linguistic heritage, presents unique challenges for automatic speech recognition (ASR) due to its phonemic complexity and the phonetic transformations that occur at word junctures, similar to the connected speech found in natural conversations. Due to these complexities, there has been limited exploration of ASR in Sanskrit, particularly in the context of its poetic verses, which are characterized by intricate prosodic and rhythmic patterns. This gap in research raises the question: How can we develop an effective ASR system for Sanskrit, particularly one that captures the nuanced features of its poetic form? In this study, we introduce Vedavani, the first comprehensive ASR study focused on Sanskrit Vedic poetry. We present a 54-hour Sanskrit ASR dataset, consisting of 30,779 labelled audio samples from the Rig Veda and Atharva Veda. This dataset captures the precise prosodic and rhythmic features that define the language. We also benchmark the dataset on various state-of-the-art multilingual speech models. Experimentation revealed that IndicWhisper performed the best among the SOTA models.","Sujeet Kumar, Pretam Ray, Abhinay Beerukuri, Shrey Kamoji, Manoj Balaji Jagadeeshan, Pawan Goyal",2025/5/30,Journal,arXiv preprint arXiv:2506.00145,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=5eQgpf8AAAAJ&citation_for_view=5eQgpf8AAAAJ:2osOgNQ5qMEC
REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback,"Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.","Aniruddha Roy, Pretam Ray, Abhilash Nandy, Somak Aditya, Pawan Goyal",2025/5/10,Journal,arXiv preprint arXiv:2505.06548,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=5eQgpf8AAAAJ&citation_for_view=5eQgpf8AAAAJ:d1gkVwhDpl0C
From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses,"Recent advances in large language models (LLMs) have significantly improved natural language generation, including creative tasks like poetry composition. However, most progress remains concentrated in high-resource languages. This raises an important question: Can LLMs be adapted for structured poetic generation in a low-resource, morphologically rich language such as Sanskrit? In this work, we introduce a dataset designed for translating English prose into structured Sanskrit verse, with strict adherence to classical metrical patterns, particularly the Anushtub meter. We evaluate a range of generative models-both open-source and proprietary-under multiple settings. Specifically, we explore constrained decoding strategies and instruction-based fine-tuning tailored to metrical and semantic fidelity. Our decoding approach achieves over 99% accuracy in producing syntactically valid poetic forms, substantially outperforming general-purpose models in meter conformity. Meanwhile, instruction-tuned variants show improved alignment with source meaning and poetic style, as supported by human assessments, albeit with marginal trade-offs in metrical precision.","Manoj Balaji Jagadeeshan, Samarth Bhatia, Pretam Ray, Harshul Raj Surana, Priya Mishra, Annarao Kulkarni, Ganesh Ramakrishnan, Prathosh AP, Pawan Goyal",2025/6/1,Journal,arXiv preprint arXiv:2506.00815,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=7jZFRqMAAAAJ&citation_for_view=7jZFRqMAAAAJ:3fE2CSJIrl8C
Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents,"The study presents a comprehensive benchmark for retrieving Sanskrit documents using English queries, focusing on the chapters of the Srimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR), Translation-based Retrieval (DT), and Query Translation (QT), utilizing shared embedding spaces and advanced translation methods to enhance retrieval systems in a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's linguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT, Contriever, and GPT-2. It adapts summarization techniques for Sanskrit documents to improve QA processing. Evaluation shows DT methods outperform DR and QT in handling the cross-lingual challenges of ancient texts, improving accessibility and understanding. A dataset of 3,400 English-Sanskrit query-document pairs underpins the study, aiming to preserve Sanskrit scriptures and share their philosophical importance widely. Our dataset is publicly available at https://huggingface.co/datasets/manojbalaji1/anveshana","Manoj Balaji Jagadeeshan, Prince Raj, Pawan Goyal",2025/5/26,Journal,arXiv preprint arXiv:2505.19494,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=7jZFRqMAAAAJ&citation_for_view=7jZFRqMAAAAJ:MXK_kJrjxJIC
